#' Génération d'une explication contextuelle avec un LLM (Ollama)
#'
#' Cette fonction envoie un prompt à un modèle LLM local via Ollama
#' et retourne une explication textuelle basée sur les résultats
#' statistiques et le contexte fourni.
#'
#' @param summary_text Texte décrivant les résultats statistiques
#' (par exemple, sortie de \code{summary.chisq_table_context()}
#' @param context Pour décrire des données et donner des informations sur les variables par exemple
 
